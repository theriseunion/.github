**English** | [中文](https://github.com/theriseunion/.github/blob/main/profile/README_zh.md)

## RiseUnion

[**RiseUnion**](https://www.theriseunion.com/)  (Beijing RiseUnion Technology Co., Ltd.)is dedicated to building an efficient and flexible AI heterogeneous computing management platform. We provide enterprises with GPU and AI compute resource pooling, scheduling, and optimization solutions to maximize resource utilization, reduce compute costs, and accelerate the growth of China's AI ecosystem.

## Project HAMi
<img src="https://github.com/Project-HAMi/HAMi/raw/master/imgs/hami-horizontal-colordark.png" width="600px">

HAMi is an open-source, cloud-native AI compute management project, officially a sandbox and [landscape](https://landscape.cncf.io/?item=orchestration-management--scheduling-orchestration--hami) project of  
[Cloud Native Computing Foundation](https://cncf.io/)(CNCF), 
[CNAI Landscape project](https://landscape.cncf.io/?group=cnai&item=cnai--general-orchestration--hami).

It is designed to enable **efficient AI compute orchestration and intelligent scheduling**, leveraging the CNCF ecosystem for deeper integration with Kubernetes and containerized computing platforms.


As a core contributor to the [HAMi open-source community](https://github.com/Project-HAMi), RiseUnion is actively advancing HAMi to establish an open and flexible heterogeneous AI compute virtualization middleware. HAMi supports a wide range of domestic and international AI accelerators, delivering high-performance virtualization and intelligent resource scheduling capabilities.

## **HAMi Key Features**

### 1. Heterogeneous GPU Resource Pooling
- Supports **NVIDIA GPUs** and domestic AI accelerators such as **Ascend, Metax, Iluvatar, Cambricon, Hygon, and Mthreads**.
- Provides Virtual GPU (vGPU) and Multi-Instance GPU (MIG) support to enable efficient compute resource management.

### 2. Cross-Architecture Unified Scheduling
- Compatible with x86 and ARM server architectures.
- Distributed scheduling framework for large-scale GPU workload management.

### 3. Intelligent Compute Scheduling & Optimization
- Implements priority-based scheduling, preemptive task allocation, and dynamic scaling.
- Leverages time-slice to maximize GPU utilization.
- Optimized for inference workloads, supporting **DeepSeek, Qwen, Llama**, and other large models.

### 4. Cloud-Native Integration
- Fully Kubernetes-native, enabling seamless GPU workload scheduling within K8s clusters.
- Easily integrates with PyTorch, TensorFlow, and other AI training and inference frameworks.

### 5. WebUI for Compute Management
- A modern **WebUI** for real-time GPU resource monitoring, workload management, and policy configuration.  
  Explore [HAMi WebUI](https://github.com/Project-HAMi/HAMi-WebUI).
- Provides a visualized compute pool overview, simplifying GPU cluster operations.

### 6. Open Community & Ecosystem Collaboration
- Jointly developed and maintained by **RiseUnion, 4Paradigm, Huawei, DaoCloud**, and other key industry players.
- Brings together AI compute providers and chip manufacturers to drive standardization of AI compute infrastructure.
- As a CNCF Incubating Project, HAMi fosters an open ecosystem for enterprise AI compute management.


## Join the HAMi Community
HAMi is an open-source project, and we welcome developers, researchers, and enterprises to contribute and collaborate in building a more efficient AI compute orchestration ecosystem.

### How to Contribute:  
- **Code Contributions**: Submit a Pull Request to enhance HAMi's features or fix issues.  
- **Technical Discussions**: Share feedback and suggestions on [GitHub Issues](https://github.com/Project-HAMi/HAMi/issues).  
- **Documentation Improvements**: Help improve HAMi's documentation for better adoption.  
- **Community Collaboration**: Explore new possibilities in AI compute resource management.

For more information, visit [HAMi GitHub](https://github.com/Project-HAMi).
